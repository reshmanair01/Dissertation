# -*- coding: utf-8 -*-
"""BERTBiLSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1POgm9mVKjSjln9r8YavzkT2fKxjxPRkd

# Load Modules -
"""

!pip install transformers

!pip install umap-learn

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf

"""# Sarcasam Dataset PreProcessing - Initial"""

# Define a function to load and extract the required column from the given file
def extract_text_column(file_path, class_name):
    with open(file_path, 'r', encoding='utf-8') as file:
        first_line = file.readline()

    # Infer the delimiter based on the first line
    delimiters = [',', ';', '\t', '|']
    delimiter_counts = {delim: first_line.count(delim) for delim in delimiters}
    most_common_delim = max(delimiter_counts, key=delimiter_counts.get)

    df = pd.read_csv(file_path, delimiter=most_common_delim)

    # Assuming the tweet text is in the 5th column (0-based index)
    df = df.iloc[:, 4:5]
    df.columns = ["text"]
    df["class"] = class_name

    return df

data_files = ['sarcasm.csv',
 'irony_test.csv',
 '.DS_Store',
 'readme.txt',
 'regular_test.csv',
 'irony.csv',
 'figurative.csv',
 'sarcasm_test.csv',
 'figurative_test.csv',
 'regular.csv']

# Filter out non-CSV files and those not ending with "_test.csv"
relevant_files = [f for f in data_files if f.endswith(".csv") and not f.endswith("_test.csv")]

# Extract and merge data from each file
merged_df = pd.concat([extract_text_column(f"{file_name}", file_name.split(".")[0])
                       for file_name in relevant_files], ignore_index=True)
merged_df.head()

merged_df.to_csv("tweet_sarcasm.csv")

"""# Load the datasets -"""

product_data = pd.read_csv('product_company_tweet.csv', encoding='ISO-8859-1')
sarcasm_data = pd.read_csv('tweet_sarcasm.csv')

product_data.info()

product_data.head(2)

sarcasm_data.info()

sarcasm_data.head(2)

"""# Pre-Processing -"""

# Add a new column to each dataframe to specify the context
product_data['context'] = 'product'
sarcasm_data['context'] = 'general'

product_data.head(2)

# Merge the two dataframes
merged_data = pd.concat([product_data, sarcasm_data])

merged_data.info()

merged_data.head(5)

# Merge the two dataframes
merged_data = pd.concat([product_data, sarcasm_data])

# Fill missing values with empty strings
merged_data['tweet_text'].fillna('', inplace=True)
merged_data['tweets'].fillna('', inplace=True)

# Generate the 'all_text' column
merged_data['all_text'] = merged_data['tweet_text'] + ' ' + merged_data['tweets']

# Make sure to shuffle your data if needed
merged_data = merged_data.sample(frac=1).reset_index(drop=True)

merged_data.head(5)

# checking null values in all_text because it is having all tweets from product and sarcasm dataset
merged_data['all_text'].isna().sum()

merged_data['is_there_an_emotion_directed_at_a_brand_or_product'].unique()

merged_data['class'].unique()

num_nan = merged_data['class'].isna().sum()
print(f"There are {num_nan} 'nan' values in the 'class' column.")

nan_rows = merged_data[merged_data['class'].isna()]
print(nan_rows)

regular_rows = merged_data[merged_data['class'] == 'regular']
print(regular_rows) ## Filling regular on all the classes with nan values because all are related to product category

"""## Data Cleaning"""

import re

# Based on the above result fro column 'is_there_an_emotion_directed_at_a_brand_or_product'
# replacing {'nan','I can't tell'} with 'No emotion toward brand or product'

# Fill 'nan' values with 'No emotion toward brand or product'
merged_data['is_there_an_emotion_directed_at_a_brand_or_product'].fillna('No emotion toward brand or product', inplace=True)

# Replace "I can't tell" values with 'No emotion toward brand or product'
merged_data['is_there_an_emotion_directed_at_a_brand_or_product'].replace("I can't tell", 'No emotion toward brand or product', inplace=True)

## Filling regular on all the classes with nan values because all are related to product category
# Replace 'nan' values with 'regular'
merged_data['class'].fillna('regular', inplace=True)

# Function to remove URLs
def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

# Apply the function to your text data
merged_data['all_text'] = merged_data['all_text'].apply(remove_urls)

import re

# Function to remove hashtags
def remove_hashtags(text):
    return re.sub(r'#\w+', '', text)

# Apply the function to your text data
merged_data['all_text'] = merged_data['all_text'].apply(remove_hashtags)

merged_data.head(5)

"""# EDA"""

import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

import pandas as pd

tweet_column_name = 'all_text'

# Calculate the length of each tweet and store it in a new column 'tweet_length'.
merged_data['tweet_length'] = merged_data[tweet_column_name].apply(len)

# Calculate the average tweet length.
average_tweet_length = merged_data['tweet_length'].mean()

# Print the average tweet length.
print(f'Average Tweet Length: {average_tweet_length:.2f} characters')

import matplotlib.pyplot as plt

# Calculate the class distribution
class_counts = merged_data['class'].value_counts()

# Create a pie chart
plt.figure(figsize=(8, 8))
plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Class Labels')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.

# Show the pie chart
plt.show()

# Plot for class labels
plt.figure(figsize=(10, 5))
sns.countplot(data=merged_data, x='class')
plt.title('Distribution of Class')
plt.show()

# Plot for context labels
plt.figure(figsize=(10, 5))
sns.countplot(data=merged_data, x='context')
plt.title('Distribution of Context')
plt.show()

# Filter data for sarcastic and non-sarcastic tweets
sarcastic_tweets = merged_data[merged_data['class'] == 'Sarcastic']
non_sarcastic_tweets = merged_data[merged_data['class'] == 'Non-sarcastic']

# Create separate box plots for sarcastic and non-sarcastic tweet lengths
plt.figure(figsize=(8, 6))
sns.boxplot(data=merged_data, x='class', y='tweet_length', palette=['skyblue', 'salmon'])
plt.title('Box Plot of Text Length by Class')
plt.xlabel('Class')
plt.ylabel('Text Length')
plt.show()

pip install nltk

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.probability import FreqDist
import matplotlib.pyplot as plt
nltk.download('punkt')
# Assuming 'merged_data' is your DataFrame containing the 'all_text' column
all_text = merged_data['all_text']

# Tokenize the text (split into words)
words = nltk.word_tokenize(" ".join(all_text))

# Download stopwords if not already downloaded
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Filter out stopwords
filtered_words = [word.lower() for word in words if word.isalnum() and word.lower() not in stop_words]

# Calculate word frequency
freq_dist = FreqDist(filtered_words)

# Print most common words
print("Most common words:")
print(freq_dist.most_common(10))  # Change '10' to your desired number

# Plot frequency distribution
plt.figure(figsize=(10, 5))
freq_dist.plot(30, cumulative=False)
plt.title('Frequency Distribution of Words')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.show()

# Download the stopwords dataset
nltk.download('stopwords')

# Get the list of stopwords for English
stop_words = set(stopwords.words('english'))

# Print the list of stopwords
print(stop_words)

from wordcloud import WordCloud
import matplotlib.pyplot as plt
# Combine all text into one big text
text = ' '.join(text for text in merged_data['all_text'])

# Filter out the stopwords from the text
filtered_text = ' '.join(word for word in text.split() if word.lower() not in stop_words)

# Create a word cloud
wordcloud = WordCloud(background_color='white').generate(filtered_text)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

## Checking the most common words list
# Combine all text into one big text
text = ' '.join(text for text in merged_data['all_text'])

# Create a word cloud
wordcloud = WordCloud(background_color='white').generate(text)

# Display the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

merged_data.info()

"""# Preparing data for modeling -

### Text Normalization
"""

# Convert to lowercase
merged_data['all_text'] = merged_data['all_text'].str.lower()

# Remove punctuation
merged_data['all_text'] = merged_data['all_text'].str.replace('[^\w\s]','')

"""# Model"""

!pip install transformers

import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from transformers import BertTokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from transformers import MobileBertTokenizer

tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')

# Tokenize the text and create input IDs
input_ids = [tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True) for text in merged_data['all_text']]

# Create attention masks
attention_masks = [[1] * len(input_id) for input_id in input_ids]

# Pad the sequences
input_ids = pad_sequences(input_ids, maxlen=512, dtype='int32', padding='post', truncating='post', value=0)
attention_masks = pad_sequences(attention_masks, maxlen=512, dtype='int32', padding='post', truncating='post', value=0)

print("Shape of input_ids:", input_ids.shape)
print("Shape of attention_masks:", attention_masks.shape)

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Create a label encoder for 'context'
le_context = LabelEncoder()
context_encoded = le_context.fit_transform(merged_data['context'])

# Create a label encoder for 'class'
le_class = LabelEncoder()
class_encoded = le_class.fit_transform(merged_data['class'])

from sklearn.model_selection import train_test_split

# Assuming `context_encoded` and `class_encoded` are your encoded labels
labels = list(zip(context_encoded, class_encoded))

# Split the data into training and validation sets
train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)
train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=42, test_size=0.1)

# Separate the context and class labels
train_context, train_class = zip(*train_labels)
validation_context, validation_class = zip(*validation_labels)

from transformers import TFMobileBertModel
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dropout, Dense, Bidirectional, LSTM
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Adam

# Define the inputs
input_ids = Input(shape=(512,), dtype='int32')
attention_masks = Input(shape=(512,), dtype='int32')

# Load the MobileBERT model
bert_model = TFMobileBertModel.from_pretrained('google/mobilebert-uncased')

# Pass the inputs through the BERT model
bert_output = bert_model([input_ids, attention_masks])[0]

# Add a BiLSTM layer
lstm_output = Bidirectional(LSTM(128, return_sequences=False))(bert_output)

# Add dropout for regularization
lstm_output = Dropout(0.1)(lstm_output)

# Add a Dense layer for each classification task
context_output = Dense(len(le_context.classes_), activation='softmax', name='context_output')(lstm_output)
class_output = Dense(len(le_class.classes_), activation='softmax', name='class_output')(lstm_output)

# Define the model
model = Model(inputs=[input_ids, attention_masks], outputs=[context_output, class_output])

from transformers import AdamWeightDecay, get_cosine_schedule_with_warmup

# Set up the learning rate schedule
initial_learning_rate = 2e-5
decay_steps = 10000
decay_rate = 0.96
lr_schedule = ExponentialDecay(
    initial_learning_rate,
    decay_steps=decay_steps,
    decay_rate=decay_rate,
    staircase=True)

# Initialize the Adam optimizer with learning rate schedule
optimizer = Adam(learning_rate=lr_schedule)

from tensorflow.keras.callbacks import ModelCheckpoint

# Create a callback that saves the model's weights
checkpoint = ModelCheckpoint(filepath="/content/drive/My Drive/checkpoint.ckpt",
                             save_weights_only=True,
                             save_best_only=True,
                             monitor='val_loss',
                             verbose=1)


# Compile the model
model.compile(
    optimizer=optimizer,
    loss={'context_output': 'sparse_categorical_crossentropy', 'class_output': 'sparse_categorical_crossentropy'},
    metrics={'context_output': 'accuracy', 'class_output': 'accuracy'}
)

# Display the model architecture
model.summary()

from google.colab import drive

# Convert the targets to numpy arrays
train_context = np.array(train_context)
validation_context = np.array(validation_context)
train_class = np.array(train_class)
validation_class = np.array(validation_class)

history = model.fit(
    [train_inputs, train_masks],
    [train_context, train_class],
    validation_data=([validation_inputs, validation_masks], [validation_context, validation_class]),
    batch_size=20,
    epochs=10, callbacks=[checkpoint]
)

model.save('/content/drive/My Drive/twitter_multiclass_model.h5')

# Load the checkpoint weights
model.load_weights('/content/drive/My Drive/checkpoint.ckpt')

# Convert the targets to numpy arrays
train_context = np.array(train_context)
validation_context = np.array(validation_context)
train_class = np.array(train_class)
validation_class = np.array(validation_class)

# Continue training for 4 more epochs (total 7 epochs)
history = model.fit(
    [train_inputs, train_masks],
    [train_context, train_class],
    validation_data=([validation_inputs, validation_masks], [validation_context, validation_class]),
    batch_size=20,
    epochs=7,  # Train for 4 more epochs (total 7 epochs)
    callbacks=[checkpoint]  # Assuming you have defined the checkpoint callback
)

"""# Model Valdation"""

!pip install transformers

"""Run load and preprocessing - modules above"""

from tensorflow.keras.models import load_model
from transformers import MobileBertTokenizer
from transformers import TFMobileBertModel
from sklearn.preprocessing import LabelEncoder

model = load_model('/content/drive/My Drive/twitter_multiclass_model (1).h5',
                   custom_objects={'TFMobileBertModel': TFMobileBertModel})

# Load the tokenizer
tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')

# Sample text
texts = ['I dont think any TV show could be more #Epic than #MasterShowman! This show is the bees knees!!']

# Preprocess the text
inputs = tokenizer(texts, return_tensors='tf', padding='max_length', truncation=True, max_length=512)

# Get the input IDs and attention masks
input_ids = inputs['input_ids']
attention_masks = inputs['attention_mask']

# Make predictions
predictions = model.predict([input_ids, attention_masks])

# The model returns probabilities for each class. To get the actual classes, we can use the argmax function.
context_predictions = np.argmax(predictions[0], axis=1)
class_predictions = np.argmax(predictions[1], axis=1)

# Create the encoders
le_context = LabelEncoder()
le_class = LabelEncoder()

# Fit the encoders
le_context.fit(merged_data['context'])
le_class.fit(merged_data['class'])

# Convert the indices back to their original labels
context_labels = le_context.inverse_transform(context_predictions)
class_labels = le_class.inverse_transform(class_predictions)

print("Context Predictions:", context_labels)
print("Class Predictions:", class_labels)

"""# Analysis"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import load_model
from transformers import MobileBertTokenizer
from transformers import TFMobileBertModel
import pandas as pd
from tensorflow.keras.models import Model
from tensorflow.keras.models import load_model
from transformers import MobileBertTokenizer
from transformers import TFMobileBertModel
from sklearn.preprocessing import LabelEncoder
from sklearn.manifold import TSNE
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import umap

model = load_model('/content/drive/My Drive/twitter_multiclass_model (1).h5',
                   custom_objects={'TFMobileBertModel': TFMobileBertModel})

# Load the tokenizer
tokenizer = MobileBertTokenizer.from_pretrained('google/mobilebert-uncased')

from tensorflow.keras.models import Model
import numpy as np

# Create a new model to extract word-level embeddings
word_embedding_layer = model.get_layer(name='bidirectional').output
word_embedding_model = Model(inputs=model.input, outputs=word_embedding_layer)

# Sample a subset of the data
subset_size = 1000
subset = merged_data.sample(subset_size)

# Tokenize and preprocess the subset
input_ids_subset = [tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True) for text in subset['all_text']]
attention_masks_subset = [[1] * len(input_id) for input_id in input_ids_subset]
input_ids_subset = pad_sequences(input_ids_subset, maxlen=512, dtype='int32', padding='post', truncating='post', value=0)
attention_masks_subset = pad_sequences(attention_masks_subset, maxlen=512, dtype='int32', padding='post', truncating='post', value=0)

# Convert input data to TensorFlow tensors
input_ids_subset = tf.constant(input_ids_subset)
attention_masks_subset = tf.constant(attention_masks_subset)

# Generate word-level embeddings for the subset
word_embeddings_subset = word_embedding_model.predict([input_ids_subset, attention_masks_subset])

# Store the embeddings (e.g., save them to a file)
np.save('word_embeddings_subset.npy', word_embeddings_subset)

from google.colab import drive
drive.mount('/content/drive')

import matplotlib.patches as mpatches
# Load the word embeddings
embeddings = word_embeddings_subset

# Decode the tokenized input_ids back to words using the tokenizer
decoded_texts = [tokenizer.decode(input_id, skip_special_tokens=True) for input_id in input_ids_subset]

# Flatten the decoded texts to generate labels for each word embedding
flat_labels = ' '.join(decoded_texts).split()

# Extract class labels from the original dataset (assuming 'class' column contains labels)
class_labels = subset['class'].values  # Modify this according to your dataset structure

# Limit the number of embeddings and labels for visualization
max_labels = 500
flat_word_embeddings = embeddings.reshape(-1, embeddings.shape[-1])
flat_word_embeddings = flat_word_embeddings[:max_labels]
flat_labels = flat_labels[:max_labels]
class_labels = class_labels[:max_labels]  # Ensure it's of the same length as flat_labels

# Reduce the dimensionality of the flattened embeddings using PCA
pca = PCA(n_components=2)
embeddings_2d = pca.fit_transform(flat_word_embeddings)

# Define a color map for different classes (you can customize this)
color_map = {
    'sarcasm': 'red',
    'irony': 'blue',
    'figurative': 'green',
    'regular': 'orange'
}

# Create a list of colors corresponding to each data point based on class
colors = [color_map[label] for label in class_labels]

plt.figure(figsize=(20, 15))
scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5, c=colors)


# # Plotting the embeddings with labels and color-coded classes
# plt.figure(figsize=(20, 15))
# plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.5, c=colors)
# Define custom legend labels based on your class names
legend_labels = [mpatches.Patch(color=color, label=label) for label, color in color_map.items()]

# Add the legend to the right side of the plot
plt.legend(handles=legend_labels, bbox_to_anchor=(1.05, 1), loc='upper left')


# # Annotate each point with its corresponding word and class
# for i, (word, label) in enumerate(zip(flat_labels, class_labels)):
#     plt.annotate(f"{word} ({label})", (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=8, alpha=0.7)

plt.title("2D Visualization of Word Embeddings with Color-Coded Classes")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

merged_data.to_csv('merged_data.csv')

# Load the embeddings and dataset
embeddings = np.load('word_embeddings_subset.npy')
dataset = pd.read_csv('merged_data.csv', engine='python')

from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from transformers import MobileBertTokenizer
import tensorflow as tf
import numpy as np
import pandas as pd
import umap
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


# Create a new model to extract sentence-level embeddings
sentence_embedding_layer = model.get_layer(name='bidirectional').output
sentence_embedding_model = Model(inputs=model.input, outputs=sentence_embedding_layer)

# Load dataset
merged_data = pd.read_csv('merged_data.csv', engine='python')

# Sample a subset of the data
subset_size = 1000
subset = merged_data.sample(subset_size)

# Tokenize and preprocess the subset
input_ids_subset = [tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True) for text in subset['all_text']]
attention_masks_subset = [[1] * len(input_id) for input_id in input_ids_subset]
input_ids_subset = pad_sequences(input_ids_subset, maxlen=512, dtype='int32', padding='post', truncating='post', value=0)
attention_masks_subset = pad_sequences(attention_masks_subset, maxlen=512, dtype='int32', padding='post', truncating='post', value=0)

# Convert input data to TensorFlow tensors
input_ids_subset = tf.constant(input_ids_subset)
attention_masks_subset = tf.constant(attention_masks_subset)

# Generate sentence-level embeddings for the subset
sentence_embeddings_subset = sentence_embedding_model.predict([input_ids_subset, attention_masks_subset])

# UMAP transformation
reducer = umap.UMAP(n_components=3, random_state=42)
embeddings_3d_umap = reducer.fit_transform(sentence_embeddings_subset)

# Visualization
fig = plt.figure(figsize=(12, 10))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(embeddings_3d_umap[:, 0], embeddings_3d_umap[:, 1], embeddings_3d_umap[:, 2], c='b', alpha=0.6)
ax.set_title("3D UMAP Visualization of Sentence Embeddings")
plt.show()

import plotly.express as px
import pandas as pd

# Convert the UMAP embeddings to a DataFrame
df_umap = pd.DataFrame(embeddings_3d_umap, columns=['UMAP_X', 'UMAP_Y', 'UMAP_Z'])
df_umap['sentence'] = subset['all_text'].values
df_umap['label'] = subset['class'].values  # Assuming 'class' column contains labels like sarcasm, irony, etc.

# Create an interactive 3D scatter plot using Plotly with color-coded categories
fig = px.scatter_3d(df_umap, x='UMAP_X', y='UMAP_Y', z='UMAP_Z', color='label', hover_name='sentence', opacity=0.5)
fig.update_traces(marker=dict(size=5))
fig.update_layout(title="3D UMAP Visualization of Sentence Embeddings with Labels")
fig.show()



"""# BiLSTM"""

from tensorflow.keras.layers import Embedding
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dropout, Dense, Bidirectional, LSTM, Embedding
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Assuming vocabulary size and embedding dimension
VOCAB_SIZE = 30522  # Adjust based on your tokenizer's vocabulary or input features
EMBEDDING_DIM = 128

# Define the inputs
input_ids = Input(shape=(512,), dtype='int32')

# Embedding layer
embedding_layer = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM)(input_ids)

# Add a BiLSTM layer
lstm_output = Bidirectional(LSTM(128, return_sequences=False))(embedding_layer)

# Add dropout for regularization
lstm_output = Dropout(0.1)(lstm_output)

# Add a Dense layer for each classification task
context_output = Dense(len(le_context.classes_), activation='softmax', name='context_output')(lstm_output)
class_output = Dense(len(le_class.classes_), activation='softmax', name='class_output')(lstm_output)

# Define the model
bilstm_model = Model(inputs=input_ids, outputs=[context_output, class_output])

# Compile the model
bilstm_model.compile(
    loss={'context_output': 'sparse_categorical_crossentropy', 'class_output': 'sparse_categorical_crossentropy'},
    metrics={'context_output': 'accuracy', 'class_output': 'accuracy'}
)

# Convert the targets to numpy arrays
train_context = np.array(train_context)
validation_context = np.array(validation_context)
train_class = np.array(train_class)
validation_class = np.array(validation_class)

# Train the BiLSTM-only model
history_bilstm = bilstm_model.fit(
    train_inputs,  # No need for masks since we're not using BERT
    [train_context, train_class],
    validation_data=(validation_inputs, [validation_context, validation_class]),
    batch_size=30,
    epochs=10
)

model.save("/content/drive/My Drive/twitter_bilstm_model.h5")

"""# BERT"""

!pip install transformers

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf

import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.layers import Embedding
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dropout, Dense
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Add a new column to each dataframe to specify the context
product_data['context'] = 'product'
sarcasm_data['context'] = 'general'

# Merge the two dataframes
merged_data = pd.concat([product_data, sarcasm_data])

# Merge the two dataframes
merged_data = pd.concat([product_data, sarcasm_data])

# Fill missing values with empty strings
merged_data['tweet_text'].fillna('', inplace=True)
merged_data['tweets'].fillna('', inplace=True)

# Generate the 'all_text' column
merged_data['all_text'] = merged_data['tweet_text'] + ' ' + merged_data['tweets']

# Make sure to shuffle your data if needed
merged_data = merged_data.sample(frac=1).reset_index(drop=True)

# Based on the above result fro column 'is_there_an_emotion_directed_at_a_brand_or_product'
# replacing {'nan','I can't tell'} with 'No emotion toward brand or product'

# Fill 'nan' values with 'No emotion toward brand or product'
merged_data['is_there_an_emotion_directed_at_a_brand_or_product'].fillna('No emotion toward brand or product', inplace=True)

# Replace "I can't tell" values with 'No emotion toward brand or product'
merged_data['is_there_an_emotion_directed_at_a_brand_or_product'].replace("I can't tell", 'No emotion toward brand or product', inplace=True)

## Filling regular on all the classes with nan values because all are related to product category
# Replace 'nan' values with 'regular'
merged_data['class'].fillna('regular', inplace=True)

import re
# Function to remove URLs
def remove_urls(text):
    url_pattern = re.compile(r'https?://\S+|www\.\S+')
    return url_pattern.sub(r'', text)

# Apply the function to your text data
merged_data['all_text'] = merged_data['all_text'].apply(remove_urls)

import re

# Function to remove hashtags
def remove_hashtags(text):
    return re.sub(r'#\w+', '', text)

# Apply the function to your text data
merged_data['all_text'] = merged_data['all_text'].apply(remove_hashtags)

# Convert to lowercase
merged_data['all_text'] = merged_data['all_text'].str.lower()

# Remove punctuation
merged_data['all_text'] = merged_data['all_text'].str.replace('[^\w\s]','')

import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.layers import Embedding
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dropout, Dense, Bidirectional, LSTM, Embedding
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

# Tokenize the text and create input IDs
input_ids = [tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True) for text in merged_data['all_text']]

# Create attention masks
attention_masks = [[1] * len(input_id) for input_id in input_ids]

# Pad the sequences
input_ids = pad_sequences(input_ids, maxlen=512, dtype='int32', padding='post', truncating='post', value=0)
attention_masks = pad_sequences(attention_masks, maxlen=512, dtype='int32', padding='post', truncating='post', value=0)

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Create a label encoder for 'context'
le_context = LabelEncoder()
context_encoded = le_context.fit_transform(merged_data['context'])

# Create a label encoder for 'class'
le_class = LabelEncoder()
class_encoded = le_class.fit_transform(merged_data['class'])

from sklearn.model_selection import train_test_split

# Assuming `context_encoded` and `class_encoded` are your encoded labels
labels = list(zip(context_encoded, class_encoded))

# Split the data into training and validation sets
train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=42, test_size=0.1)
train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=42, test_size=0.1)

# Separate the context and class labels
train_context, train_class = zip(*train_labels)
validation_context, validation_class = zip(*validation_labels)

from transformers import TFBertForSequenceClassification, BertTokenizer
from tensorflow.keras.optimizers import Adam

# Load MobileBERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('google/mobilebert-uncased')
mobilebert_model = TFBertForSequenceClassification.from_pretrained('google/mobilebert-uncased', num_labels=len(le_context.classes_))

# Define the model
input_ids = Input(shape=(512,), dtype='int32')
attention_mask = Input(shape=(512,), dtype='int32')

outputs = mobilebert_model(input_ids, attention_mask=attention_mask)
context_output = Dense(len(le_context.classes_), activation='softmax', name='context_output')(outputs[0])
class_output = Dense(len(le_class.classes_), activation='softmax', name='class_output')(outputs[0])

model = Model(inputs=[input_ids, attention_mask], outputs=[context_output, class_output])

# Compile the model
model.compile(
    loss={'context_output': 'sparse_categorical_crossentropy', 'class_output': 'sparse_categorical_crossentropy'},
    metrics={'context_output': 'accuracy', 'class_output': 'accuracy'},
    optimizer=Adam(learning_rate=5e-5)
)

# Convert the targets to numpy arrays
train_context = np.array(train_context)
validation_context = np.array(validation_context)
train_class = np.array(train_class)
validation_class = np.array(validation_class)

# Train the MobileBERT model
history_mobilebert = model.fit(
    [train_inputs, train_masks],
    [train_context, train_class],
    validation_data=([validation_inputs, validation_masks], [validation_context, validation_class]),
    batch_size=20,
    epochs=10
)

from sklearn.metrics import classification_report
predictions = model.predict([validation_inputs, validation_masks])
predicted_context = np.argmax(predictions[0], axis=1)
predicted_class = np.argmax(predictions[1], axis=1)

# Classification report for "context"
print("Classification Report for Context:")
print(classification_report(validation_context, predicted_context, target_names=le_context.classes_))


# Classification report for "class"
print("Classification Report for Class:")
print(classification_report(validation_class, predicted_class, target_names=le_class.classes_))

model.save("/content/drive/My Drive/twitter_bert_model.h5")